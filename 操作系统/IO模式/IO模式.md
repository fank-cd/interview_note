# IO模式
概念说明
  * 用户空间和内核空间：
    * 操作系统的核心是内核，独立于普通的应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的所有权限。为了保证用户进程不能直接操作内核（kernel），保证内核的安全，操作系统将虚拟空间划分为两部分，一部分为内核空间，一部分为用户空间
  * 进程切换：
    * 为了控制进程的执行，内核必须有能力挂起正在CPU上运行的进程，并恢复以前挂起的某个进程的执行。这种行为被称为进程切换
  * 进程的阻塞 ：
    *  正在执行的进程，由于期待的某些事件未发生，如请求系统资源失败、等待某种操作的完成、新数据尚未到达或无新工作做等，则由系统自动执行阻塞原语(Block)，使自己由运行状态变为阻塞状态，当进程进入阻塞状态，是不占用CPU资源的
  * 文件描述符 ：
    * 是一个用于表述指向文件的引用的抽象化概念。文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表
  * 缓存 IO:
    * 缓存 IO 又被称作标准 IO，大多数文件系统的默认 IO 操作都是缓存 IO。在 Linux 的缓存 IO 机制中，操作系统会将 IO 的数据缓存在文件系统的页缓存（ page cache ）中，也就是说，数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。






Linux 五种IO模型:
  * 网络IO的本质是socket的读取，socket在linux系统被抽象为流，IO可以理解为对流的操作。对于一次IO访问（以read举例），数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。所以说，当一个read操作发生时，它会经历两个阶段：
    1. 等待数据准备
    2. 将数据从内核拷贝到进程中
  * 对于socket流而言，
    1. 通常涉及等待网络上的数据分组到达，然后被复制到内核的某个缓冲区。
    2.  把数据从内核缓冲区复制到应用进程缓冲区
  * 网络IO模型有如下几种：
    * 同步阻塞 IO
      * 在linux中，默认情况下所有的socket都是blocking
      * 在这个IO模型中，用户空间的应用程序执行一个系统调用（recvform），这会导致应用程序阻塞，什么也不干，直到数据准备好，并且将数据从内核复制到用户进程，最后进程再处理数据，在等待数据到处理数据的两个阶段，整个进程都被阻塞。不能处理别的网络IO。调用应用程序处于一种不再消费 CPU 而只是简单等待响应的状态
      *

    * 同步非阻塞 IO
      * 同步非阻塞是轮询（polling）方式。在这种模型中，设备是以非阻塞的形式打开的
      * 非阻塞的recvform系统调用调用之后，进程并没有被阻塞，内核马上返回给进程，如果数据还没准备好，此时会返回一个error。进程在返回之后，可以干点别的事情，然后再发起recvform系统调用。重复上面的过程，循环往复的进行recvform系统调用。这个过程通常被称之为轮询。轮询检查内核数据，直到数据准备好，再拷贝数据到进程，进行数据处理。需要注意，拷贝数据整个过程，进程仍然是属于阻塞的状态
      *

    * IO多路复用：
      * 将同步非阻塞 IO中的用户态进程的轮询，放到内核态中，就是IO多路复用
      * 相比非阻塞IO，可以同时监听多个Socket，当其中任何一个socket的数据准好了，就能返回进行可读，然后进程再进行recvform系统调用，将数据由内核拷贝到用户进程，当然这个过程是阻塞的
      * select或poll调用之后，会阻塞进程，与blocking IO阻塞不同在于，此时的select不是等到socket数据全部到达再处理, 而是有了一部分数据就会调用用户进程来处理。如何知道有一部分数据到达了呢？监视的事情交给了内核，内核负责数据到达的处理
    * 异步IO（asynchronous IO）：
      * 相对于同步IO，异步IO不是顺序执行。用户进程进行aio_read系统调用之后，无论内核数据是否准备好，都会直接返回给用户进程，然后用户态进程可以去做别的事情。等到socket数据准备好了，内核直接复制数据给进程，然后从内核向进程发送通知。IO两个阶段，进程都是非阻塞的




IO多路复用


  * select
    * select 将已连接的 Socket 都放到一个文件描述符集合，然后调用 select 函数将文件描述符集合拷贝到内核里，让内核来检查是否有网络事件产生。内核通过遍历文件描述符集合的方式，当检查到有事件产生后，将此 Socket 标记为可读或可写， 接着再把整个文件描述符集合拷贝回用户态里，然后用户态还需要再通过遍历的方法找到可读或可写的 Socket，然后再对其处理
    * select 使用固定长度的 BitsMap，表示文件描述符集合，而且所支持的文件描述符的个数是有限制的，在 Linux 系统中，由内核中的 FD_SETSIZE 限制， 默认最大值为 1024，只能监听 0~1023 的文件描述符
    * 时间复杂度：时间复杂度为 O(n)
    * select目前几乎在所有的平台上支持
  * poll
    * poll本质上和select没有区别，它将用户传入的数组拷贝到内核空间，然后查询每个fd对应的设备状态，如果设备就绪则在设备等待队列中加入一项并继续遍历，如果遍历完所有fd后没有发现就绪设备，则挂起当前进程，直到设备就绪或者主动超时，被唤醒后它又要再次遍历fd。这个过程经历了多次无谓的遍历。
    * poll 通过使用链表来存储fd，所支持的文件描述符没有限制
    * 时间复杂度：O(n)
    * poll使用水平触发
  * epoll：
    * epoll 在内核里使用红黑树来跟踪进程所有待检测的文件描述字，把需要监控的 socket 通过 epoll_ctl() 函数加入内核中的红黑树里，红黑树增删查一般时间复杂度是 O(logn)，通过对这棵黑红树进行操作，只需要传入一个待检测的 socket，减少了内核和用户空间大量的数据拷贝和内存分配。
    * epoll 使用事件驱动的机制，内核里维护了一个链表来记录就绪事件，当某个 socket 有事件发生时，通过回调函数内核会将其加入到这个就绪事件列表中，当用户调用 epoll_wait() 函数时，只会返回有事件发生的文件描述符的个数，不需要像 select/poll 那样轮询扫描整个 socket 集合，大大提高了检测的效率。
    * 没有最大并发连接的限制
    * 支持水平触发与边缘触发
      * 水平触发：LT(level triggered)是缺省的工作方式，并且同时支持block和no-block socket
        * 当被监控的 Socket 上有可读事件发生时，服务器端不断地从 epoll_wait 中苏醒，直到内核缓冲区数据被 read 函数读完才结束，目的是告诉我们有数据需要读取；
      * 边缘触发：ET(edge-triggered)是高速工作方式，只支持no-block socket
        * 当被监控的 Socket 描述符上有可读事件发生时，服务器端只会从 epoll_wait 中苏醒一次，即使进程没有调用 read 函数从内核读取数据，也依然只苏醒一次，因此我们程序要保证一次性将内核缓冲区的数据读取完
        * 边缘触发模式一般和非阻塞 I/O 搭配使用
        * ET模式在很大程度上减少了epoll事件被重复触发的次数，因此效率要比LT模式高
  * select 和 poll 的缺陷在于，当客户端越多，也就是 Socket 集合越大，Socket 集合的遍历和拷贝会带来很大的开销，
  * 如果没有大量的idle-connection或者dead-connection，epoll的效率并不会比select/poll高很多，但是当遇到大量的idle-connection，就会发现epoll的效率大大高于select/poll

| select| poll| epoll
---|---|---
索引就绪文件描述符的时间复杂度| O(n)| O(n)| O(1)
最大支持文件描述符数| 1024| 没有最大连接数限制| 没有最大连接数限制
工作模式| LT| LT| ET
内核实现| 轮询| 轮询| 采用回调方式来检测就绪事件
优点| 跨平台| 理论上没有最大数量限制（数量过大性能急剧下降）| 没有描述符限制IO效率不会随着监视fd数量增长而下降
缺点| 有最大描述符限制| 索引就绪文件描述符的时间复杂度过高| 在所有socket都很活跃的情况下，可能会有性能问题