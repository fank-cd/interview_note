# 数据库


1、简述乐观锁以及悲观锁的区别以及使用场景
  * 乐观锁和悲观锁其实都是并发控制的机制，同时它们在原理上就有着本质的差别；
    * 乐观锁是一种思想，它其实并不是一种真正的『锁』，它会先尝试对资源进行修改，在写回时判断资源是否进行了改变，如果没有发生改变就会写回，否则就会进行重试，在整个的执行过程中其实都没有对数据库进行加锁；
    * 悲观锁就是一种真正的锁了，它会在获取资源前对资源进行加锁，确保同一时刻只有有限的线程能够访问该资源，其他想要尝试获取资源的操作都会进入等待状态，直到该线程完成了对资源的操作并且释放了锁后，其他线程才能重新操作资源；
  * 乐观锁不会存在死锁的问题，但是由于更新后验证，所以当冲突频率和重试成本较高时更推荐使用悲观锁
  * 而需要非常高的响应速度并且并发量非常大的时候使用乐观锁就能较好的解决问题，在这时使用悲观锁就可能出现严重的性能问题；




2、MySQL 为什么使用 B+ 树来作索引，对比 B 树它的优点和缺点是什么？
  * B树
    1. 1个m阶的B树具有如下几个特征：
      1. 根结点至少有两个子女。
      2. 每个中间节点都包含k-1个元素和k个孩子，其中 m/2 <= k <= m
      3. 每一个叶子节点都包含k-1个元素，其中 m/2 <= k <= m
      4. 所有的叶子结点都位于同一层。
      5. 每个节点中的元素从小到大排列，节点当中k-1个元素正好是k个孩子包含的元素的值域分划。
    1. 多路搜索树，非二叉树
    2. 每个节点既保存索引，又保存数据
    3. 搜索时相当于二分查找
    4. 为什么设计成多路：
      * 降低树的高度，树的高度和查找的时间复杂度相关
    5. 可以设计成无限多路吗：
      * 不行，退化为有序数组
    6. 应用场景：文件系统、MongoDb索引
    7. 为什么用B树而不是红黑树或者有数数组呢：
      * 索引很大的情况下，内存不能将索引一次读完，需要磁盘IO操作，最坏情况下，磁盘IO操作次数等于索引树的高度，所以多路的B树会将树变得“矮胖”而减少磁盘IO
      * 在内存中红黑树是优于B树的
  * B+树
    1. 1个M阶的B+树具有如下几个特征：
      1. 有k个子树的中间节点包含有k个元素，每个元素不保存数据，只用来索引，所有数据保存在叶子节点
      2. 所有的叶子节点包含了全部元素的信息，及指向这些元素记录的指针，且叶子节点本身依关键字的大小自小而大顺序连接
      3. 所有的中间节点元素都同时存在于子节点，在子节点元素中是最大或者最小元素
      4. 每个叶子节点都带有指向下一个节点的指针，形成了一个有序链表
    2. 多路搜索树，非二叉树
    3. 只有叶子节点保存数据
    4. 搜索时相当于二分查找
    5. 增加了相邻接点的指向指针。
    6. 应用场景：Mysql
    7. 为什么不用哈希表：
      * 哈希虽然能够提供 O(1) 的单数据行操作性能，但是对于范围查询和排序却无法很好地支持，最终导致全表扫描；
  * B+树优点：
    * B+树 对比 B树没有中间节点没有存数据，所以同样大小的磁盘页可以容纳更多的节点元素，同数据量下磁盘IO更少
    * B+树由于必须找到叶子节点，所以查找性能稳定，而B-树可能在中间节点，最优为O(1),最劣为O(logn)
    * 针对范围检索时，B树必须要做中序遍历，而B+树可以直接做O（n）的链表遍历
  * B树优点：
    * 因为B树节点中包含数据，经常访问的节点可以更靠近根节点，因为可以更快地访问
  * MySQL 为什么使用 B+ 树来作索引：
    * 硬盘的索引一般在磁盘上，数据量大的情况下无法一次性装入内存，B+树的设计允许数据分批加载，同时降低了树的高度，对比 B树没有中间节点没有存数据，所以同样大小的磁盘页可以容纳更多的节点元素，同数据量下磁盘IO更少。提高查找效率
    * 在数据库范围查询中,B+树索引有序，而且又有链表相连，B树必须要做中序遍历，而B+树可以直接做O（n）的链表遍历，速度很快。






 3、什么是数据库事务，MySQL 为什么会使用 InnoDB 作为默认选项
  * 数据库事务：多条语句作为一个整体进行操作的功能，可以确保该事务范围内的所有操作都可以全部成功或者全部失败
    * 特性：
      * A：Atomic，原子性，将所有SQL作为原子工作单元执行，要么全部执行，要么全部不执行；
      * C：Consistent，一致性，事务完成后，所有数据的状态都是一致的，即A账户只要减去了100，B账户则必定加上了100；
      * I：Isolation，隔离性，如果有多个事务并发执行，每个事务作出的修改必须与其他事务隔离；
      * D：Duration，持久性，即事务完成后，对数据库数据的修改被持久化存储。
    * 隐式事务：对于单条SQL语句，数据库系统自动将其作为一个事务执行
    * 显式事务：手动把多条SQL语句作为一个事务执行，使用BEGIN开启一个事务，使用COMMIT提交一个事务，这种事务被称为
    * ROLLBACK回滚事务
  * MySQL为什么会使用InnoDB作为默认选项：
    * InnoDB 是聚集索引，数据文件是和索引绑在一起的，必须要有主键，通过主键索引效率很高，但是辅助索引需要两次查询，先查询到主键，然后再通过主键查询到数据。因此，主键不应该过大，否则其他索引也会很大。而 MyISAM 是非聚集索引，数据文件是分离的，索引保存的是数据文件的指针，主键索引和辅助索引是独立的。
    * InnoDB 支持外键，而 MyISAM 不支持。对一个包含外键的 InnoDB 表转为 MYISAM 会失败。
    * InnoDB 在 MySQL 5.6 之前不支持全文索引，而 MyISAM 一直都支持，如果你用的是老版本，查询效率上 MyISAM 要高。
    * InnoDB 锁粒度是行锁，而 MyISAM 是表锁。
    * InnoDB 支持事务，MyISAM 不支持，对于 InnoDB 每一条 SQL 语言都默认封装成事务，自动提交，这样会影响速度，所以最好把多条 SQL 语言放在 begin 和 commit 之间，组成一个事务。
    * InnoDB 不保存表的具体行数，执行 select count(*) from table 时需要全表扫描。而 MyISAM 用一个变量保存了整个表的行数，执行上述语句时只需要读出该变量即可，速度很快，但如果上述语句还包含了 where 子句，那么两者执行效率是一样的。
  * 如何选择：
    * 是否要支持事务，如果要请选择 Innodb，如果不需要可以考虑 MyISAM。
    * 如果表中绝大多数都是读查询（有人总结出 读:写比率大于100:1），可以考虑 MyISAM，如果既有读又有写，而且也挺频繁，请使用 InnoDB。
    * 系统崩溃后，MyISAM 恢复起来更困难，能否接受。


4、数据库的事务隔离级别有哪些？各有哪些优缺点？
  * 读未提交RAED UNCOMMITED：使用查询语句不会加锁，可能会读到未提交的行（Dirty Read）；
  * 读已提交READ COMMITED：只对记录加记录锁，而不会在记录之间加间隙锁，所以允许新的记录插入到被锁定记录的附近，所以再多次使用查询语句时，可能得到不同的结果（Non-Repeatable Read）
  * 可重复读REPEATABLE READ：多次读取同一范围的数据会返回第一次查询的快照，不会返回不同的数据行，但是可能发生幻读（Phantom Read）；
  * 串行化SERIALIZABLE：InnoDB 隐式地将全部的查询语句加上共享锁，解决了幻读的问题；




MySQL 中默认的事务隔离级别就是 REPEATABLE READ，但是它通过 Next-Key 锁也能够在某种程度上解决幻读的问题。


几种锁：
  * 共享锁（读锁）：允许事务对一条行数据进行读取
  * 互斥锁（写锁）：允许事务对一条行数据进行删除或更新
  * 锁的粒度：
    * 行锁
    * 表锁：
      * 意向锁 ：是否有人请求锁定表中的某一行数据。
        * 意向共享锁：事务想要在获得表中某些记录的共享锁，需要在表上先加意向共享锁；
        * 意向互斥锁：事务想要在获得表中某些记录的互斥锁，需要在表上先加意向互斥锁；




锁的算法：
  * 记录锁（Record Lock）是加到索引记录上的锁
  * 间隙锁 (Gap Lock)是对索引记录中的一段连续区域的锁；当使用类似 SELECT * FROM users WHERE id BETWEEN 10 AND 20 FOR UPDATE; 的 SQL 语句时，就会阻止其他事务向表中插入 id = 15 的记录，因为整个范围都被间隙锁锁定了,它唯一阻止的就是其他事务向这个范围中添加新的记录。


脏读、幻读、不可重复读
  * 脏读：在一个事务中，读取了其他事务未提交的数据。
  * 不可重复读：在一个事务中，同一行记录被访问了两次却得到了不同的结果。原因是在 READ COMMITED 的隔离级别下，存储引擎不会在查询记录时添加行锁，锁定 id = 3 这条记录
  * 幻读：在一个事务中，同一个范围内的记录被读取时，其他事务向这个范围添加了新的记录。可由更高的隔离级别 SERIALIZABLE 解决的，但是它也可以通过 MySQL 提供的 Next-Key锁解决




5、什么情况下会发生死锁，如何解决死锁？
  
* 资源：大部分的死锁都和资源有关，在进程对设备、文件具有独占性（排他性）时会产生死锁。我们把这类需要排他性使用的对象称为资源(resource)。资源主要分为 可抢占资源和不可抢占资源
* 可抢占资源和不可抢占资源:

	* 可抢占资源(preemptable resource) 可以从拥有它的进程中抢占而不会造成其他影响，内存就是一种可抢占性资源，任何进程都能够抢先获得内存的使用权。
	* 不可抢占资源(nonpreemtable resource) 指的是除非引起错误或者异常，否则进程无法抢占指定资源，这种不可抢占的资源比如有打印机，在进程执行调度的过程中，其他进程是不能得到该资源的


	* 死锁的概念：是指两个或两个以上的进程在执行过程中，由于竞争资源或者由于彼此通信而造成的一种阻塞的现象，若无外力作用，它们都将无法推进下去。
	* 死锁产生的原因：
		* 竞争可消耗资源引起死锁:系统资源在分配时出现失误,进程间对资源的相互争夺而造成僵局.
		* 进程推进顺序不当引起死锁 :多道程序运行时,进程推进顺序不合理


	* 产生死锁的必要条件：

		* 互斥条件：一个资源每次只能被一个进程使用。
		* 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。
		* 不剥夺条件：进程已获得的资源，在末使用完之前，不能强行剥夺。
		* 循环等待条件：若干进程之间形成一种头尾相接的循环等待资源关系。
	* 死锁的预防  ：死锁的预防是保证系统不进入死锁状态的一种策略。它的基本思想是要求进程申请资源时遵循某种协议，从而打破产生死锁的四个必要条件中的一个或几个，保证系统不会进入死锁状态。

		* 破坏互斥条件。即允许进程同时访问某些资源。但是，有的资源是不允许被同时访问的，像打印机等等，这是由资源本身的属性所决定的。所以，这种办法并无实用价值。
		* 破坏不可剥夺条件。即允许进程强行从占有者那里夺取某些资源。就是说，当一个进程已占有了某些资源，它又申请新的资源，但不能立即被满足时，它必须释放所占有的全部资源，以后再重新申请。它所释放的资源可以分配给其它进程。这就相当于该进程占有的资源被隐蔽地强占了。这种预防死锁的方法实现起来困难，会降低系统性能
		* 破坏请求与保持条件。可以实行资源预先分配策略。即进程在运行前一次性地向系统申请它所需要的全部资源。如果某个进程所需的全部资源得不到满足，则不分配任何资源，此进程暂不运行。只有当系统能够满足当前进程的全部资源需求时，才一次性地将所申请的资源全部分配给该进程。由于运行的进程已占有了它所需的全部资源，所以不会发生占有资源又申请资源的现象，因此不会发生死锁。

			* 缺点：

				* 在许多情况下，一个进程在执行之前不可能知道它所需要的全部资源。这是由于进程在执行时是动态的，不可预测的
				* 资源利用率低。无论所分资源何时用到，一个进程只有在占有所需的全部资源后才能执行。即使有些资源最后才被该进程用到一次，但该进程在生存期间却一直占有它们，造成长期占着不用的状况。这显然是一种极大的资源浪费
				* 降低了进程的并发性。因为资源有限，又加上存在浪费，能分配到所需全部资源的进程个数就必然少了
		* 破坏循环等待条件，实行资源有序分配策略。采用这种策略，即把资源事先分类编号，按号分配，使进程在申请，占用资源时不会形成环路。所有进程对资源的请求必须严格按资源序号递增的顺序提出。进程占用了小号资源，才能申请大号资源，就不会产生环路，从而预防了死锁。这种策略与前面的策略相比，资源的利用率和系统吞吐量都有很大提高

			* 缺点：

				* 限制了进程对资源的请求，同时给系统中所有资源合理编号也是件困难事，并增加了系统开销
				* 为了遵循按编号申请的次序，暂不使用的资源也需要提前申请，从而增加了进程对资源的占用时间
	* 死锁的避免：

		* 安全序列：  我们首先引入安全序列的定义：所谓系统是安全的，是指系统中的所有进程能够按照某一种次序分配资源，并且依次地运行完毕，这种进程序列{P1，P2，...，Pn}就是安全序列。如果存在这样一个安全序列，则系统是安全的；如果系统不存在这样一个安全序列，则系统是不安全的
		* 安全序列{P1，P2，...，Pn}是这样组成的：若对于每一个进程Pi，它需要的附加资源可以被系统中当前可用资源加上所有进程Pj当前占有资源之和所满足，则{P1，P2，...，Pn}为一个安全序列，这时系统处于安全状态，不会进入死锁状态。 　
		* 银行家算法
	* 死锁的检测与恢复

		* 最简单，最常用的方法就是进行系统的重新启动，不过这种方法代价很大，它意味着在这之前所有的进程已经完成的计算工作都将付之东流，包括参与死锁的那些进程，以及未参与死锁的进程。
		* 撤消进程，剥夺资源。终止参与死锁的进程，收回它们占有的资源，从而解除死锁。这时又分两种情况：一次性撤消参与死锁的全部进程，剥夺全部资源；或者逐步撤消参与死锁的进程，逐步收回死锁进程占有的资源。一般来说，选择逐步撤消的进程时要按照一定的原则进行，目的是撤消那些代价最小的进程，比如按进程的优先级确定进程的代价；考虑进程运行时的代价和与此进程相关的外部作业的代价等因素。
		* 进程回退策略，即让参与死锁的进程回退到没有发生死锁前某一点处，并由此点处继续执行，以求再次执行时不再发生死锁。虽然这是个较理想的办法，但是操作起来系统开销极大，要有堆栈这样的机构记录进程的每一步变化，以便今后的回退，有时这是无法做到的。
		* 看门狗计数器:当线程正常运行的时候会每隔一段时间重置计数器，在没有发生死锁的情况下，一切都正常进行。一旦发生死锁，由于无法重置计数器导致定时器超时，这时程序会通过重启自身恢复到正常状态。


死锁代码：

```

死锁代码
要AB两个锁
当A锁内部需要使用B锁，同时B锁内部需要使用A锁的时候，就要可能会出现死锁
出现死所的解决方法：
1.重构代码
2.添加timeout时间
下面代码中要可能返回下面内容，在同时认为锁被对方占用，同时释放
Thread-1获取到a锁
Thread-2获取到b锁
over
"""
import time
from threading import Thread, Lock


lock_a = Lock()
lock_b = Lock()


class MyThread(Thread):
    def run(self) -> None:
        if lock_a.acquire():  # 如果a锁可以获取到，返回True,获取不到就阻塞
            print(self.name + "获取到a锁")
            time.sleep(0.1)
            # if lock_b.acquire(timeout=0.01):  # 这里不加退出时间就会要可能出现死锁
            if lock_b.acquire():  # 这里不加退出时间就会要可能出现死锁
                print(self.name + "获取到b锁，现有ab锁")
                lock_b.release()
            lock_a.release()
class MyThread1(Thread):
    def run(self) -> None:
        if lock_b.acquire():  # 如果b锁可以获取到，返回True,获取不到就阻塞
            print(self.name + "获取到b锁")
            time.sleep(0.1)
            if lock_b.acquire():  # 这里不加退出时间就会要可能出现死锁
                print(self.name + "获取到a锁，现有ab锁")
                lock_a.release()
            lock_b.release()
if __name__ == '__main__':
    t1 = MyThread()
    t2 = MyThread1()
    t1.start()
    t2.start()
    t1.join()
    t2.join()
    print("over")
```
6、简述脏读和幻读的发生场景，InnoDB 是如何解决幻读的？
  * 脏读： 在一个事务中，读取了其他事务未提交的数据。
    * 场景：当事务的隔离级别为 READ UNCOMMITED 时，我们在 SESSION 2 中插入的未提交数据在 SESSION 1 中是可以访问的。
    * 解决：提高事务隔离级别 READ COMMITED
    * 原因：在RAED UNCOMMITED事务隔离级别中，使用查询语句不会加锁




| SESSION 1| SESSION 2
---|---|---
| BEGING| BEGING
 |	id	|+-------+| SELECT * FROM test|
| | INSERT INTO test VALUES(1)
 |	id	|+	1	+| SELECT * FROM test|
  * 不可重复读： 在一个事务中，同一行记录被访问了两次却得到了不同的结果。
    * 场景：当事务的隔离级别为 READ COMMITED 时在 SESSION 1 先查询了一行数据，在这之后 SESSION 2 中修改了同一行数据并且提交了修改，在这时，如果 SESSION 1 中再次使用相同的查询语句，就会发现两次查询的结果不一样。
    * 解决：提高事务的隔离级别
    * 原因：原因就是，在 READ COMMITED 的隔离级别下，存储引擎不会在查询记录时添加行锁，锁定 id = 3 这条记录。

| SESSION 1| SESSION 2
---|---|---
| BEGING| BEGING
 |	id   |  v  |+	3   |  2  |+| SELECT * FROM test WHERE id = 3|
| | UPDATE test set v = 3 WHERE id = 3
| | COMMIT
 |	id   |  v  |+	3   |  3  |+| SELECT * FROM test WHERE id = 3|






  * 幻读： 在一个事务中，同一个范围内的记录被读取时，其他事务向这个范围添加了新的记录。
    * 场景: 重新开启了两个会话 SESSION 1 和 SESSION 2，在 SESSION 1 中我们查询全表的信息，没有得到任何记录；在 SESSION 2 中向表中插入一条数据并提交；由于 REPEATABLE READ 的原因，再次查询全表的数据时，我们获得到的仍然是空集, 但是在向表中插入同样的数据却出现了错误。
    * 解决: 幻读是由更高的隔离级别 SERIALIZABLE 解决的，但是它也可以通过 MySQL 提供的 Next-Key 锁解决
    * 原因 :REPEATABLE READ 和 READ UNCOMMITED 其实是矛盾的，如果保证了前者就看不到已经提交的事务，如果保证了后者，就会导致两次查询的结果不同，MySQL 为我们提供了一种折中的方式，能够在 REPEATABLE READ 模式下加锁访问已经提交的数据，其本身并不能解决幻读的问题，而是通过文章前面提到的 Next-Key 锁来解决

| SESSION 1| SESSION 2
---|---|---
| BEGING| BEGING
 |	id	|+-------+| SELECT * FROM test|
| | INSERT INTO test VALUES(1)
| | COMMIT
 |	id	|+	1	+| SELECT * FROM test|
| INSERT INTO test VALUES(1)|
| Dunlicate entry '1' for  'id'|
  * InnoDB 是如何解决幻读的 : 幻读是由更高的隔离级别 SERIALIZABLE 解决的，但是它也可以通过 MySQL 提供的 Next-Key 锁解决
  * Next-Key: Next-Key 锁锁定的是当前值和前面的范围
    * 举例：
      * 当我们更新一条记录，比如 SELECT * FROM users WHERE age = 30 FOR UPDATE;，InnoDB 不仅会在范围 (21, 30] 上加 Next-Key 锁，还会在这条记录后面的范围 (30, 40] 加间隙锁，所以插入 (21, 40] 范围内的记录都会被锁定








7、简述数据库中的 ACID 分别是什么？
  * A：Atomic，原子性，将所有SQL作为原子工作单元执行，要么全部执行，要么全部不执行；
  * C：Consistent，一致性，事务完成后，所有数据的状态都是一致的，即A账户只要减去了100，B账户则必定加上了100；
  * I：Isolation，隔离性，如果有多个事务并发执行，每个事务作出的修改必须与其他事务隔离；
  * D：Duration，持久性，即事务完成后，对数据库数据的修改被持久化存储。




8、并发事务会引发哪些问题？如何解决？
  * 脏读
    * 解决：提高事务隔离级别 READ COMMITED
  * 幻读
    * 解决: 幻读是由更高的隔离级别 SERIALIZABLE 解决的，但是它也可以通过 MySQL 提供的 Next-Key 锁解决
  * 不可重复读
    * 解决：提高事务的隔离级别
  * （具体见第六题）


 9、简述 Redis 持久化中 rdb 以及 aof 方案的优缺点
  * RDB：快照形式是直接把内存中的数据保存到一个 dump 文件中，定时保存，保存策略。Redis默认是快照RDB的持久化方式
  * AOF：把所有的对Redis的服务器进行修改的命令都存到一个文件里，命令的集合。、
  * 当 Redis 重启时，它会优先使用 AOF 文件来还原数据集，因为 AOF 文件保存的数据集通常比 RDB 文件所保存的数据集更完整。




RDB持久化：
  * 工作原理：当 Redis 需要做持久化时，Redis 会 fork 一个子进程，子进程将数据写到磁盘上一个临时 RDB 文件中。当子进程完成写临时文件后，将原来的 RDB 替换掉，这样的好处就是可以 copy-on-write。
  * Redis.conf配置

```
save 900 1 # 900秒之内，如果超过1个key被修改，则发起快照保存
save 300 10 # 300秒内，如果超过10个key被修改，则发起快照保存
save 60 10000 # 1分钟之内，如果1万个key被修改，则发起快照保存
```
  * 优点：
    * 这种文件非常适合用于进行备份： 比如说，你可以在最近的 24 小时内，每小时备份一次 RDB 文件，并且在每个月的每一天，也备份一个 RDB 文件。 这样的话，即使遇上问题，也可以随时将数据集还原到不同的版本。RDB 非常适用于灾难恢复（disaster recovery）。
    * 灵活设置备份的频率和周期
    * 性能最大化：对于 Redis 的服务进程而言，在开始持久化时，它唯一需要做的只是 fork 出子进程，之后再由子进程完成这些持久化的工作，这样就可以极大的避免服务进程执行 IO 操作了。也就是说，RDB 对 Redis 对外提供的读写服务，影响非常小，可以让 Redis 保持高性能
    * 恢复更快速：相比于 AOF 机制，RDB 的恢复速度更更快，更适合恢复数据，特别是在数据集非常大的情况。
  * 缺点：
    * 如果你需要尽量避免在服务器故障时丢失数据，那么 RDB 不适合你。 虽然 Redis 允许你设置不同的保存点（save point）来控制保存 RDB 文件的频率， 但是， 因为RDB 文件需要保存整个数据集的状态， 所以它并不是一个轻松的操作。 因此你可能会至少 5 分钟才保存一次 RDB 文件。 在这种情况下， 一旦发生故障停机， 你就可能会丢失好几分钟的数据。
    * RDB备份机制，是通过一个fork子进程来进行协助数据的持久化，在持久化的过程中，将会暂停所有的redis的数据集的操作，如果redis的数据集庞大，那么将有可能导致redis停止对外服务几百毫秒，或者是1秒




AOF 持久化：
  * 工作原理：使用 AOF 做持久化，每一个写命令都通过write函数追加到 appendonly.aof 中
  * Redis.conf配置

```
appendfsync yes
appendfsync always     #每次有数据修改发生时都会写入AOF文件。
appendfsync everysec   #每秒钟同步一次，该策略为AOF的缺省策略。
```
  * 优点：
    * 使用 AOF 持久化会让 Redis 变得非常耐久（much more durable）：你可以设置不同的 fsync 策略，比如无 fsync ，每秒钟一次 fsync ，或者每次执行写入命令时 fsync 。 AOF 的默认策略为每秒钟 fsync 一次，在这种配置下，Redis 仍然可以保持良好的性能，并且就算发生故障停机，也最多只会丢失一秒钟的数据（ fsync 会在后台线程执行，所以主线程可以继续努力地处理命令请求）。
    * AOF是基于日志模式记录数据操作，所以该操作可以更加好的保证数据的安全性。
    * AOF日志写入记录操作模式为 append 模式，所以在使用过程中，即使服务器出现的宕机情况，也不会破坏redis 的日志文件内容。
    * AOF 包含一个格式清晰、易于理解的日志文件用于记录所有的修改操作
  * 缺点：
    * 对于相同的数据集来说，AOF 文件的体积通常要大于 RDB 文件的体积。根据所使用的 fsync 策略，AOF 的速度可能会慢于 RDB。 在一般情况下， 每秒 fsync 的性能依然非常高， 而关闭 fsync 可以让 AOF 的速度和 RDB 一样快， 即使在高负荷之下也是如此。 不过在处理巨大的写入载入时，RDB 可以提供更有保证的最大延迟时间（latency）
    * 针对相同数据量大小的恢复数据操作，建议使用RDB恢复，因相同数据大小情况下，RDB恢复速度比AOF恢复更快。
    * 根据同步策略的不同，AOF 在运行效率上往往会慢于 RDB 。总之，每秒同步策略的效率是比较高的，同步禁用策略的效率和 RDB 一样高效。




两者的区别：
  * RDB持久化是指在指定的时间间隔内将内存中的数据集快照写入磁盘，实际操作过程是fork一个子进程，先将数据集写入临时文件，写入成功后，再替换之前的文件，用二进制压缩存储。
  * AOF持久化以日志的形式记录服务器所处理的每一个写、删除操作，查询操作不会记录，以文本的方式记录，可以打开文件看到详细的操作记录。




Redis 为什么在使用 RDB 进行快照时会通过子进程的方式进行实现？
  1. 通过 fork 创建的子进程能够获得和父进程完全相同的内存空间，父进程对内存的修改对于子进程是不可见的，两者不会相互影响；
  2. 通过 fork 创建子进程时不会立刻触发大量内存的拷贝，内存在被修改时会以页为单位进行拷贝，这也就避免了大量拷贝内存而带来的性能问题；




10、简述 Redis 的哨兵机制
在 Redis 中，实现 高可用 的技术主要包括 持久化、复制、哨兵 和 集群，下面简单说明它们的作用，以及解决了什么样的问题：
  * 持久化：持久化是 最简单的 高可用方法。它的主要作用是 数据备份，即将数据存储在 硬盘，保证数据不会因进程退出而丢失。
  * 复制：复制是高可用 Redis 的基础，哨兵 和 集群 都是在 复制基础 上实现高可用的。复制主要实现了数据的多机备份以及对于读操作的负载均衡和简单的故障恢复。缺陷是故障恢复无法自动化、写操作无法负载均衡、存储能力受到单机的限制。
  * 哨兵：在复制的基础上，哨兵实现了 自动化 的 故障恢复。缺陷是 写操作 无法 负载均衡，存储能力 受到 单机 的限制。
  * 集群：通过集群，Redis 解决了 写操作 无法 负载均衡 以及 存储能力 受到 单机限制 的问题，实现了较为 完善 的 高可用方案。






Redis主从复制的问题
  1. 一旦 主节点宕机，从节点 作为 主节点 的 备份 可以随时顶上来。
  2. 扩展 主节点 的 读能力，分担主节点读压力。




主从复制 同时存在以下几个问题：
  1.  一旦 主节点宕机，从节点 晋升成 主节点，同时需要修改 应用方 的 主节点地址，还需要命令所有 从节点 去 复制 新的主节点，整个过程需要 人工干预。
  2. 主节点 的 写能力 受到 单机的限制。
  3. 主节点 的 存储能力 受到 单机的限制。
  4.  原生复制 的弊端在早期的版本中也会比较突出，比如：Redis 复制中断 后，从节点 会发起 psync。此时如果 同步不成功，则会进行 全量同步，主库 执行 全量备份 的同时，可能会造成毫秒或秒级的 卡顿。


基本工作原理：
我们可以将 Redis Sentinel 集群看成是一个 ZooKeeper 集群，它是集群高可用的心脏，它一般是由 3～5 个节点组成，这样挂了个别节点集群还可以正常运转。它负责持续监控主从节点的健康，当主节点挂掉时，自动选择一个最优的从节点切换为主节点。客户端来连接集群时，会首先连接 sentinel，通过 sentinel 来查询主节点的地址，然后再去连接主节点进行数据交互。当主节点发生故障时，客户端会重新向 sentinel 要地址，sentinel 会将最新的主节点地址告诉客户端。如此应用程序将无需重启即可自动完成节点切换。


Sentinel 的主要功能包括 主节点存活检测、主从运行情况检测、自动故障转移 （failover）、主从切换。Redis 的 Sentinel 最小配置是一主一从。
  * 监控:Sentinel 会不断的检查 主服务器 和 从服务器 是否正常运行。
  * 通知:当被监控的某个 Redis 服务器出现问题，Sentinel 通过 API 脚本 向 管理员 或者其他的 应用程序 发送通知。
  * 自动故障转移当 主节点 不能正常工作时，Sentinel 会开始一次 自动的 故障转移操作，它会将与 失效主节点 是 主从关系 的其中一个 从节点 升级为新的 主节点，并且将其他的 从节点 指向 新的主节点。
  * 配置提供者:在 Redis Sentinel 模式下，客户端应用 在初始化时连接的是 Sentinel 节点集合，从中获取 主节点 的信息。


主观下线和客观下线
默认情况下，每个 Sentinel 节点会以 每秒一次 的频率对 Redis 节点和 其它 的 Sentinel 节点发送 PING 命令，并通过节点的 回复 来判断节点是否在线。
  * 主观下线: 适用于所有 主节点 和 从节点。如果在 down-after-milliseconds 毫秒内，Sentinel 没有收到 目标节点 的有效回复，则会判定 该节点 为 主观下线。
  * 客观下线:只适用于 主节点。如果 主节点 出现故障，Sentinel 节点会通过 sentinel is-master-down-by-addr 命令，向其它 Sentinel 节点询问对该节点的 状态判断。如果超过 <quorum> 个数的节点判定 主节点 不可达，则该 Sentinel 节点会判断 主节点 为 客观下线。




Sentinel的通信命令
  * 连接其他哨兵：pub/sub
  * 连接redis：cmd


工作流程：
每个 Sentinel 节点都需要 定期执行 以下任务：
  1. 每个 Sentinel 以 每秒钟 一次的频率，向它所知的 主服务器、从服务器 以及其他 Sentinel 实例 发送一个 PING 命令。
  2. 如果一个 实例（instance）距离 最后一次 有效回复 PING 命令的时间超过 down-after-milliseconds 所指定的值，那么这个实例会被 Sentinel 标记为 主观下线。
  3. 如果一个 主服务器 被标记为 主观下线，那么正在 监视 这个 主服务器 的所有 Sentinel 节点，要以 每秒一次 的频率确认 主服务器 的确进入了 主观下线 状态。
  4. 如果一个 主服务器 被标记为 主观下线，并且有 足够数量 的 Sentinel（至少要达到 配置文件 指定的数量）在指定的 时间范围 内同意这一判断，那么这个 主服务器 被标记为 客观下线。
  5. 在一般情况下， 每个 Sentinel 会以每 10 秒一次的频率，向它已知的所有 主服务器 和 从服务器 发送 INFO 命令。当一个 主服务器 被 Sentinel 标记为 客观下线 时，Sentinel 向 下线主服务器 的所有 从服务器 发送 INFO 命令的频率，会从 10 秒一次改为 每秒一次。
  6. Sentinel 和其他 Sentinel 协商 主节点 的状态，如果 主节点 处于 SDOWN 状态，则投票自动选出新的 主节点。将剩余的 从节点 指向 新的主节点 进行 数据复制。
  7. 当没有足够数量的 Sentinel 同意 主服务器 下线时， 主服务器 的 客观下线状态 就会被移除。当 主服务器 重新向 Sentinel 的 PING 命令返回 有效回复 时，主服务器 的 主观下线状态 就会被移除。




消息丢失：
Redis 主从采用异步复制，意味着当主节点挂掉时，从节点可能没有收到全部的同步消息，这部分未同步的消息就丢失了。如果主从延迟特别大，那么丢失的数据就可能会特别多。Sentinel 无法保证消息完全不丢失，但是也尽可能保证消息少丢失。它有两个选项可以限制主从延迟过大。
```
min-slaves-to-write 1
min-slaves-max-lag 10
```
第一个参数表示主节点必须至少有一个从节点在进行正常复制，否则就停止对外写服务，丧失可用性。
何为正常复制，何为异常复制？这个就是由第二个参数控制的，它的单位是秒，表示如果 10s 没有收到从节点的反馈，就意味着从节点同步不正常，要么网络断开了，要么一直没有给反馈。


11、Redis 如何实现分布式锁？
Redis 锁主要利用 Redis 的 setnx 命令。
  * 加锁命令：SETNX key value，当键不存在时，对键进行设置操作并返回成功，否则返回失败。KEY 是锁的唯一标识，一般按业务来决定命名。
  * 解锁命令：DEL key，通过删除键值对释放锁，以便其他线程可以通过 SETNX 命令来获取锁。
  * 锁超时：EXPIRE key timeout, 设置 key 的超时时间，以保证即使锁没有被显式释放，锁也可以在一定时间后自动释放，避免资源被永远锁住。


伪代码如下：
```
if (setnx(key, 1) == 1){
    expire(key, 30)
    try {
        //TODO 业务逻辑
    } finally {
        del(key)
    }
}
```

上面的方法存在一些问题，比如
  1. SETNX 和 EXPIRE 非原子性：如果 SETNX 成功，在设置锁超时时间后，服务器挂掉、重启或网络问题等，导致 EXPIRE 命令没有执行，锁没有设置超时时间变成死锁。
     * 解决办法：
        *  Redis 2.6.12以上版本为set指令增加了可选参数，伪代码如下：set（key，1，30，NX）
        * Lua脚本
  2. 锁误解除：如果线程 A 成功获取到了锁，并且设置了过期时间 30 秒，但线程 A 执行时间超过了 30 秒，锁过期自动释放，此时线程 B 获取到了锁；随后 A 执行完成，线程 A 使用 DEL 命令来释放锁，但此时线程 B 加的锁还没有执行完成，线程 A 实际释放的线程 B 加的锁。
     * 解决办法
       * 通过在 value 中设置当前线程加锁的标识，在删除之前验证 key 对应的 value 判断锁是否是当前线程持有。可生成一个 UUID 标识当前线程、因为判断和释放锁是两个独立操作，不是原子性。所以这块使用lua脚本
       * 使用 lua 脚本做验证标识和解锁操作。
  3. 超时解锁导致并发：如果线程 A 成功获取锁并设置过期时间 30 秒，但线程 A 执行时间超过了 30 秒，锁过期自动释放，此时线程 B 获取到了锁，线程 A 和线程 B 并发执行。
     * 解决办法：
       * 将过期时间设置足够长，确保代码逻辑在锁释放之前能够执行完成。
       * 为获取锁的线程增加守护线程，为将要过期但未释放的锁增加有效时间。
  4. 不可重入：当线程在持有锁的情况下再次请求加锁，如果一个锁支持一个线程多次加锁，那么这个锁就是可重入的。如果一个不可重入锁被再次加锁，由于该锁已经被持有，再次加锁会失败
     * 解决办法：
       * Redis 可通过对锁进行重入计数，加锁时加 1，解锁时减 1，当计数归 0 时释放锁。在本地记录记录重入次数
  5. 无法等待锁释放：上述命令执行都是立即返回的，如果客户端可以等待锁释放就无法使用。
     * 解决办法：
       * 可以通过客户端轮询的方式解决该问题，当未获取到锁时，等待一段时间重新获取锁，直到成功获取锁或等待超时。这种方式比较消耗服务器资源，当并发量比较大时，会影响服务器的效率。
       * 另一种方式是使用 Redis 的发布订阅功能，当获取锁失败时，订阅锁释放消息，获取锁成功后释放时，发送锁释放消息


12、简述 Redis 中如何防止缓存雪崩和缓存击穿

* 缓存穿透：缓存穿透是指大量请求都查询数据库中不存在的数据，缓存和数据都查询不到数据，这使得请求每次都打到数据库上。缓存穿透往往是来自于故意攻击。

  * 解决方案：

	* 业务层数据校验：可以对请求数据进行校验，比如对于id<0的直接返回错误
	* 缓存空数据：将相应的key在redis中设置对应value为null（或者其它能反馈错误的值），这样请求到来时缓存一样可以生效。（此时可以将空对象的过期时间设置较短，否则攻击者请求大量数据库中不存在id，同样会缓存多个null值，也会带来redis使用内存剧增的问题）
	* 使用布隆过滤器：

		* 布隆过滤器：位数组、
			* 当一个元素加入过滤器时，使用多个hash函数对元素求值，并将位数组中对应位置为1；
			* 判断一个元素是否在过滤器中时，使用多个函数对元素求值，并判断位数组对应位置是否为1。如果都为1，认为元素在过滤器中；否则认为元素不在过滤器中。
* 缓存击穿：redis缓存中有一个key是大量请求同时访问的热点数据，如果突然这个key时间到了，那么大量的请求在缓存中获取不到该key，穿过缓存直接来到数据库导致数据库崩溃，这样因为单个key失效而穿过缓存到数据库称为缓存击穿
  * 解决方案：
  	* 使用互斥锁：当缓存失效时，引入一个锁，获得锁的线程去请求数据库，更新缓存，其余线程阻塞等待。
			* 设置锁过期时间，防止死锁
			* 为锁设置一个随机值，相同则删除锁，为了防止请求更新缓存的时间比锁的有效期还要长，导致在缓存更新过程中锁就失效了
  	* 永远不过期：不设置过期时间

		* 从缓存层面来看，确实没有设置过期时间，所以不会出现热点 key 过期后产生的问题，也就是“物理”不过期。
		* 从功能层面来看，为每个 value 设置一个逻辑过期时间，当发现超过逻辑过期时间后，会使用单独的线程去构建缓存。
		* 不足：会出现数据不一致的情况，这取决于应用方是否容忍这种不一致
 
* 缓存雪崩：类似缓存击穿，区别在于多条数据同时在某一时间过期
  * 解决方案：
  * 随机化过期时间：为了避免缓存同时过期，可在设置缓存时添加随机时间，这样就可以极大的避免大量的缓存同时失效。
  			
  * 二级缓存指的是除了 Redis 本身的缓存，再设置一层缓存，当 Redis 失效之后，先去查询二级缓存。
  		
  	* 缓存预热：对于即将来临的大量请求，我们可以提前走一遍系统，将数据提前缓存在Redis中，并设置不同的过期时间。
  		
  	* 保证Redis服务高可用：主从、哨兵、集群
  
伪代码：
```
public String get(key) {  
      String value = redis.get(key);  
      if (value == null) { //代表缓存值过期  
          //设置3min的超时，防止del操作失败的时候，下次缓存过期一直不能load db  
          // 使用setnx实现锁的效果
          if (redis.setnx(key_mutex, 1, 3 * 60) == 1) {  //代表设置成功  
               // 这时候只允许一个线程去load db
               value = db.get(key);  
               redis.set(key, value, expire_secs);  
               redis.del(key_mutex);  
          } else {  //这个时候代表同时候的其他线程已经load db并回设到缓存了，这时候重试获取缓存值即可  
               sleep(50);  
               get(key);  //重试  
          }  
       } else {  
         return value;        
       }  
}  
```

13、简述 Redis 的过期机制和内存淘汰策略
过期策略：
* 定时删除：在设置过期时间的同时，设置一个定时器，定时器的执行时间就是过期的时间点。

  * 优点：对内存最友好，过期的键会以最快的被删除，释放内存。
  * 缺点：对CPU时间最不友好，在大量键设置过期时间时，会创建大量的定时器，执行浪费CPU时间。
* 惰性删除：不管键是否过期，只有每次取值的时候，才检查是否过期，过期就删除。

		* 优点：对CPU时间最友好，取值时检查，只对当前键操作，不影响其他。
		* 缺点：队内存不友好，可能会存在大量过期的未被使用的键值没有删除，无用数据占用了大量内存。
* 定期删除：每隔一段时间，程序对数据库进行一次检查，过期的就删除。

	* 优点：前两种方案的折中，通过减少执行频率来减少对CPU时间的影响，通过定期删除减少了对内存的浪费。
	* 缺点：执行频率需要掌握好，不然太频繁则退化成定时删除，太少则退化成惰性删除。


Redis采用的是惰性删除和定期删除两种策略

持久化和复制对过期键的处理
* RDB持久化：

  * 主服务器：RDB文件无论是生成或载入，都会对过期键进行检查；生成时，过期键不写入；载入时，过期键会忽略。
  * 从服务器：载入时，不会检查是否过期，数据都会载入。
* AOF持久化：AOF文件写入时，键过期未删除，不影响；键过期已删除，则在AOF文件后追加DEL命令。
* AOF重写：AOF重写过程中会进行检查，过期的键忽略。
* 复制：主从模式下，由主服务器进行删除过期键，并显示的向从服务器发送DEL命令；从服务器自身不具备删除过期键值行为。



内存淘汰机制:当Redis的内存使用达到设置的内存上限时就会触发内存淘汰机制，按照特定的淘汰算法进行数据清理，释放内存。
具体的内存淘汰算法有一以下几种：
* noeviction：不淘汰，内存不足时， 新写入会报错。
* allkeys-lru：LRU，内存不足时，淘汰最近最少使用的key。
* allkeys-random：随机，内存不足时，在所有key中随机选择一个key淘汰。
* volatile-lru：过期时间内LRU，内存不足时，在设置了过期时间的key中，淘汰最近最少使用的key。
* volatile-random：过期时间内随机，内存不足时，在设置了过期时间的key中，随机选择一个key淘汰。
* volatile-ttl：更早过期时间，内存不足时，在设置了过期时间的key中，选择有更早过期时间的key淘汰。


Redis默认使用的是LRU算法

14、简述 Redis 中常见类型的底层数据结构
Redis 所有的数据结构都是以唯一的 key 字符串作为名称，然后通过这个唯一 key 值来获取相应的 value 数据。不同类型的数据结构的差异就在于 value 的结构不一样。

* string (字符串)：Redis 的字符串是动态字符串，是可以修改的字符串，内部结构实现上类似于 Java 的 ArrayList，采用预分配冗余空间的方式来减少内存的频繁分配

* 常见用途：

	* 用途示例：将用户信息结构体使用 JSON 序列化成字符串，然后将序列化后的字符串塞进 Redis 来缓存


操作示意：
```
> set name codehole OK 
> get name "codehole"
> exists name (integer) 1 
> del name (integer) 1 
> get name (nil)

# 批量键值对

> set name1 codehole OK 
> set name2 holycoder OK 
> mget name1 name2 name3  # 返回一个列表 
1) "codehole" 
2) "holycoder" 
3) (nil) 
> mset name1 boy name2 girl name3 unknown 
> mget name1 name2 name3 
1) "boy" 
2) "girl" 
3) "unknown"

# 过期和 set 命令扩展
> set name codehole 
> get name 
"codehole" 
> expire name 5 # 5s 后过期 ... # wait for 5s 
> get name 
(nil) 

> setex name 5 codehole # 5s 后过期，等价于 set+expire > get name "codehole" ...
 # wait for 5s 
> get name 
(nil) 
> setnx name codehole # 如果 name 不存在就执行 set 创建
 (integer) 1 
> get name 
"codehole" 
> setnx name holycoder 
(integer) 0 
# 因为 name 已经存在，所以 set 创建不成功
> get name "codehole" # 没有改变
```
* list (列表)：Redis 的列表相当于 Java 语言里面的 LinkedList，注意它是链表而不是数组

* 操作时间

  * 插入删除：O(1)
  * 索引 O(n)
  * 用途示例：

* 异步队列：将需要延后处理的任务结构体序列化成字符串塞进 Redis 的列表，另一个线程从这个列表中轮询数据进行处理。
* 底层实现：
  * 用 ziplist（数据较少的时候），加上prev，next指针，Redis 将链表和 ziplist 结合起来组成了 quicklist。也就是将多个 ziplist 使用双向指针串起来使用 
  * ziplist，也即是压缩列表。它将所有的元素紧挨着一起存储，分配的是一块连续的内存
* 为什么这样设计：普通的链表需要的附加指针空间太大，会比较浪费空间，而且会加重内存的碎片化
  * 好处：满足了快速的插入删除性能，又不会出现太大的空间冗余



操作示意：
```
# 右边进左边出：队列
> rpush books python java golang 
(integer) 3 
> llen books 
(integer) 3 
> lpop books 
"python" 
> lpop books 
"java" 
> lpop books 
"golang"
> lpop books
(nil)

# 右边进右边出：栈
> rpush books python java golang 
(integer) 3 
> rpop books 
"golang" 
> rpop books 
"java" 
> rpop books 
"python"
> rpop books 
(nil)

```

* hash (字典)：Redis 的字典相当于 Java 语言里面的 HashMap，它是无序字典。

* Redis 的字典的值只能是字符串
* 内部实现：数组 + 链表二维结构。第一维 hash 的数组位置碰撞时，就会将碰撞的元素使用链表串接起来。
* Rehash策略：渐进式 rehash

	* 渐进式 rehash：在 rehash 的同时，保留新旧两个 hash 结构，查询时会同时查询两个 hash 结构，然后在后续的定时任务中以及 hash 操作指令中，循序渐进地将旧 hash 的内容一点点迁移到新的 hash 结构中。当搬迁完成了，就会使用新的hash结构取而代之。当 hash 移除了最后一个元素之后，该数据结构自动被删除，内存被回收。
* 缺点：hash 结构的存储消耗要高于单个字符串，
* 优点：相比于字符串，不需要全部序列化整个对象，hash 可以对结构中的每个字段单独存储，

```
操作示意：

> hset books java "think in java" # 命令行的字符串如果包含空格，要用引号括起来 (integer) 
1 
> hset books golang "concurrency in go" (integer) 
1 
> hset books python "python cookbook" (integer)
1 
> hgetall books # entries()，key 和 value 间隔出现 
1) "java" 
2) "think in java" 
3) "golang" 
4) "concurrency in go" 
5) "python" 
6) "python cookbook" 
> hlen books 
(integer) 3 
> hget books java "think in java" 
> hset books golang "learning go programming" # 因为是更新操作，所以返回 0 
(integer) 0 
> hget books golang 
"learning go programming" 
> hmset books java "effective java" python "learning python" golang "modern golang programming" # 批量 set OK
```
  * set (集合)：Redis 的集合相当于 Java 语言里面的 HashSet，它内部的键值对是无序的唯一的。它的内部实现相当于一个特殊的字典，字典中所有的 value 都是一个值NULL

* 特点：

	* 无序
	* 唯一
	* 相当于没有值的字典
* 用途示例：存储活动中奖的用户 ID

```
操作示意：

> sadd books python 
(integer) 1 
> sadd books python # 重复 
(integer) 0 
> sadd books java golang 
(integer) 2 
> smembers books # 注意顺序，和插入的并不一致，因为 set 是无序的 
1) "java" 
2) "python" 
3) "golang" 
> sismember books java # 查询某个 value 是否存在，相当于 contains(o) 
(integer) 1
> sismember books rust 
(integer) 0 
> scard books # 获取长度相当于 count() 
(integer) 3 
> spop books # 弹出一个 "java"
```
  * zset (有序集合)：类似于 Java 的 SortedSet 和 HashMap 的结合体，一方面它是一个 set，保证了内部 value 的唯一性，另一方面它可以给每个 value 赋予一个 score，代表这个 value 的排序权重

   * 内部实现：跳表
   * 用途示例：存储学生的成绩，value 值是学生的 ID，score 是他的考试成绩。我们可以对成绩按分数进行排序就可以得到他的名次

```
操作示意：

> zadd books 9.0 "think in java" 
(integer) 1 
> zadd books 8.9 "java concurrency" 
(integer) 1 
> zadd books 8.6 "java cookbook" 
(integer) 1 
> zrange books 0 -1 # 按 score 排序列出，参数区间为排名范围 
1) "java cookbook" 
2) "java concurrency" 
3) "think in java" 
> zrevrange books 0 -1 # 按 score 逆序列出，参数区间为排名范围 
1) "think in java" 
2) "java concurrency"
3) "java cookbook" 
> zcard books # 相当于 count() 
(integer) 3 
> zscore books "java concurrency" # 获取指定 value 的 score 
"8.9000000000000004" # 内部 score 使用 double 类型进行存储，所以存在小数点精度问题 
> zrank books "java concurrency" # 排名 
(integer) 1 
> zrangebyscore books 0 8.91 # 根据分值区间遍历 zset 
1) "java cookbook" 
2) "java concurrency" 
> zrangebyscore books -inf 8.91 withscores # 根据分值区间 (-∞, 8.91] 遍历 zset，同时返回分值。inf 代表 infinite，无穷大的意思。 
1) "java cookbook" 
2) "8.5999999999999996" 
3) "java concurrency" 
4) "8.9000000000000004" 
> zrem books "java concurrency" # 删除 value 
(integer) 1 
> zrange books 0 -1 
1) "java cookbook" 
2) "think in java"
```
* 容器型数据结构的通用规则

  * create if not exists
  * drop if no elements
* 过期时间：
  * Redis 所有的数据结构都可以设置过期时间，时间到了，Redis 会自动删除相应的对象。
  * 过期的单位：单位。比如一个 hash 结构的过期是整个 hash 对象的过期，而不是其中的某个子 key。
  * tips：如果一个字符串已经设置了过期时间，然后你调用了 set 方法修改了它，它的过期时间会消失。

